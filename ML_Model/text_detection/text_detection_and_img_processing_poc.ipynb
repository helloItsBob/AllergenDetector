{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Implementation of code sequence to detect text fields on photo </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one of the possible ways to detect text fields on photo, we can use keras_ocr. It is a package for text detection and recognition with deep learning, yet we will use only text detection part to combine it with our project text recognition model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team also tried to use EAST, OpenCV, Tensorflow and Tesseract, yet decided to proceed with keras_ocr, so other trials won't be commited and pushed to the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional documentation on keras_ocr can be found here: https://keras-ocr.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common import\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for text field detection\n",
    "import keras_ocr\n",
    "import tensorflow as tf\n",
    "\n",
    "#for image processing\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.filters import threshold_local\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras-ocr text detector\n",
    "detector = keras_ocr.detection.Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image\n",
    "image = keras_ocr.tools.read('images/product_test_2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "predictions = detector.detect(images=[image])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw boxes around text\n",
    "showImage = keras_ocr.tools.drawBoxes(image, predictions, (0,255,0), 1)\n",
    "\n",
    "# show image\n",
    "plt.imshow(showImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutting detected text fiels into separate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut one of the detected text box for further processing\n",
    "tempImg = keras_ocr.tools.warpBox(image, predictions[11], margin=2)\n",
    "plt.imshow(tempImg)\n",
    "plt.show()\n",
    "\n",
    "#save box\n",
    "tf.keras.utils.save_img('images/img_before_processing.jpg', tempImg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image processing part poc**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image processing goal is to make text more readable for OCR model. We will use OpenCV for this purpose.\n",
    "In short we will:\n",
    "1. Convert image to grayscale (a must for OCR, as it recognises black letters on white background)\n",
    "2. Apply thresholding\n",
    "3. Apply dilation (and maybe erosion) to remove some noise and possibly we will need to apply some other filters to make text more readable like bilateral filter (may be skipped)\n",
    "4. Find contours\n",
    "5. Find bounding boxes for contours\n",
    "6. Crop image using bounding boxes into small pieces (letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gray(image):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    return plt.imshow(image, cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procImg = cv2.imread('images/img_before_processing.jpg')\n",
    "\n",
    "#could be that we need downscale/upscale (resize of the image) here\n",
    "#procImg = cv2.resize(procImg, dim(width, height), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "#convert to grayscale for further processing\n",
    "gray = cv2.cvtColor(procImg, cv2.COLOR_BGR2GRAY)\n",
    "plot_gray(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply gaussian blur to remove noise\n",
    "blurred = cv2.GaussianBlur(gray, (3, 3), 0, borderType=cv2.BORDER_WRAP)\n",
    "plot_gray(blurred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils\n",
    "#apply thresholding to convert to black and white\n",
    "\n",
    "T = threshold_local(blurred, 21, offset = 5, method = \"gaussian\")\n",
    "threshImg = (blurred > T).astype(\"uint8\") * 255\n",
    "plot_gray(threshImg)\n",
    "\n",
    "ret, thresh1 = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "plot_gray(thresh1)\n",
    "\n",
    "dilated = cv2.dilate(thresh1, None, iterations=1)\n",
    "plot_gray(dilated)\n",
    "\n",
    "#check if we need to inverse colors to make text black and background white\n",
    "threshImgInverse = cv2.bitwise_not(thresh1)\n",
    "plot_gray(threshImgInverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save processed image as a result\n",
    "cv2.imwrite('images/img_after_processing.jpg', threshImg)\n",
    "\n",
    "#save processed image inverse as a result\n",
    "cv2.imwrite('images/img_after_processing_inverse.jpg', threshImgInverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of bilateral filter\n",
    "temp = cv2.bilateralFilter(threshImg, 3, 100, 100, borderType=cv2.BORDER_CONSTANT)\n",
    "plot_gray(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Structuring findings for text field detection as a defined code sequence to use for further image processing </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectText (img):\n",
    "    # get predictions\n",
    "    predictions = detector.detect(images=[img])[0]\n",
    "\n",
    "    # draw boxes around text\n",
    "    image = keras_ocr.tools.drawBoxes(img, predictions, (0,255,0), 0)\n",
    "\n",
    "    # show image\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "    # return each detected box as a separate image\n",
    "    for i in range(len(predictions)):\n",
    "        temp = keras_ocr.tools.warpBox(img, predictions[i], margin=2)\n",
    "        plt.imshow(temp)\n",
    "        plt.show()\n",
    "        # save each image (uncomment to save)\n",
    "      #  tf.keras.utils.save_img('images/temp_text_{}.jpg'.format(i), temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with an image of a product label from phone camera\n",
    "imgTest = keras_ocr.tools.read('images/product_test_2.jpg')\n",
    "detectText(imgTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen - image (pre)processing could be adjusted for better detection result. Additionaly team can change thesholds for detection to eliminate cropped images with no recognisable text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
